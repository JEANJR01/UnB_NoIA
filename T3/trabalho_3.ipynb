{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==2.12.0 tensorflow-probability==0.20.0\n",
        "# !pip install d2l==1.0.3"
      ],
      "metadata": {
        "id": "11VsZeDKKr2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uISxqVSUKX0u"
      },
      "source": [
        "# Autores:\n",
        "**Gabriel Roberto (221020870) e Jean Soares (241033810)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IBMModel1 - Método estatístico"
      ],
      "metadata": {
        "id": "X3eWr4K0d840"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ry2z464bZy05"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import re\n",
        "import collections\n",
        "import math\n",
        "from random import choices\n",
        "from d2l import tensorflow as d2l\n",
        "from nltk import AlignedSent\n",
        "from nltk.translate import IBMModel1\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W3cj4W4WZ_YO"
      },
      "outputs": [],
      "source": [
        "def baixar_arquivo(url, endereco):\n",
        "    resposta = requests.get(url)\n",
        "    if resposta.status_code == requests.codes.OK:\n",
        "        with open(endereco, 'wb') as novo_arquivo:\n",
        "                novo_arquivo.write(resposta.content)\n",
        "        print(\"Download finalizado. Arquivo salvo em: {}\".format(endereco))\n",
        "    else:\n",
        "        resposta.raise_for_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwxw1RJIaFA0",
        "outputId": "0693dcf2-c4cc-4d2a-c03a-e6ee65bfca6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finalizado. Arquivo salvo em: ./raw_dataset.tgz\n"
          ]
        }
      ],
      "source": [
        "URL_arquivo = 'https://www.statmt.org/europarl/v7/pt-en.tgz'\n",
        "\n",
        "baixar_arquivo(URL_arquivo, './raw_dataset.tgz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqDFNIf6aJ-r",
        "outputId": "1130250f-8991-4ad0-fb97-78bbe5bb4ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.pt-en.en\n",
            "europarl-v7.pt-en.pt\n"
          ]
        }
      ],
      "source": [
        "# Caminho para o arquivo .tgz\n",
        "caminho_arquivo = './raw_dataset.tgz'\n",
        "\n",
        "# Abrir o arquivo .tgz\n",
        "with tarfile.open(caminho_arquivo, 'r:gz') as arquivo_tgz:\n",
        "    # Extrair todos os arquivos do .tgz\n",
        "    arquivo_tgz.extractall(path='./')\n",
        "\n",
        "    # Listar os arquivos extraídos\n",
        "    for membro in arquivo_tgz.getmembers():\n",
        "        print(membro.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK17mm3SKX0x"
      },
      "source": [
        "## Leitura do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BUZ7z-oBanHx"
      },
      "outputs": [],
      "source": [
        "class ParallelCorpus(d2l.DataModule):\n",
        "    \"\"\"The Parallel Corpus Portuguese-English dataset.\"\"\"\n",
        "    def _download(self, fname):\n",
        "        with open(fname) as f:\n",
        "            return f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aJ5vItAYfhUx",
        "outputId": "63ab97e1-cd2d-44ee-87cc-1a4ce2566b34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data = ParallelCorpus()\n",
        "raw_text_en = data._download('./europarl-v7.pt-en.en')\n",
        "raw_text_en[:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OP-iwcHLfrYJ",
        "outputId": "a00a51af-e9cd-4a2a-b62d-25d41b0f71c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reinício da sessão\\nDeclaro reaberta a sessão do Parlamento E'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "raw_text_pt = data._download('./europarl-v7.pt-en.pt')\n",
        "raw_text_pt[:60]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento e separação das sentenças"
      ],
      "metadata": {
        "id": "N3wrIN16ekvM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DHM_zy_Lapb9"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(ParallelCorpus)\n",
        "def _preprocess(self, text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl6t6Yeuf5Vq",
        "outputId": "7a6a5a44-892b-405b-ce42-4f2a0a9f27db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['resumption of the session',\n",
              " 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999 and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period',\n",
              " 'although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful',\n",
              " 'you have requested a debate on this subject in the course of the next few days during this partsession',\n",
              " 'in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union',\n",
              " 'please rise then for this minute s silence',\n",
              " 'the house rose and observed a minute s silence',\n",
              " 'madam president on a point of order',\n",
              " 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka',\n",
              " 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam who had visited the european parliament just a few months ago']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "text_en = data._preprocess(raw_text_en).split('\\n')\n",
        "\n",
        "del raw_text_en\n",
        "text_en[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDSzo-r8gGsy",
        "outputId": "45c32684-c1fd-4a3e-a95f-4a347ae082ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['reinício da sessão',\n",
              " 'declaro reaberta a sessão do parlamento europeu que tinha sido interrompida na sextafeira 17 de dezembro último e renovo todos os meus votos esperando que tenham tido boas férias',\n",
              " 'como puderam constatar o grande bug do ano 2000 não aconteceu em contrapartida os cidadãos de alguns dos nossos países foram vítimas de catástrofes naturais verdadeiramente terríveis',\n",
              " 'os senhores manifestaram o desejo de se proceder a um debate sobre o assunto nos próximos dias durante este período de sessões',\n",
              " 'entretanto gostaria  como também me foi pedido por um certo número de colegas  que observássemos um minuto de silêncio por todas as vítimas nomeadamente das tempestades nos diferentes países da união europeia que foram afectados',\n",
              " 'convidoos a levantaremse para um minuto de silêncio',\n",
              " 'o parlamento de pé guarda um minuto de silêncio',\n",
              " 'senhora presidente intervenho para um ponto de ordem',\n",
              " 'certamente que já tomou conhecimento pelas notícias transmitidas na imprensa e na televisão dos diversos atentados à bomba e assassínios perpetrados no sri lanka',\n",
              " 'uma das pessoas recentemente assassinadas foi o senhor kumar ponnambalam que ainda há poucos meses visitara o parlamento europeu']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "text_pt = data._preprocess(raw_text_pt).split('\\n')\n",
        "\n",
        "del raw_text_pt\n",
        "text_pt[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_en = [sentence.split() for sentence in text_en]\n",
        "sentences_pt = [sentence.split() for sentence in text_pt]\n",
        "del text_en, text_pt"
      ],
      "metadata": {
        "id": "rqQPjAqy3eMj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences_en), len(sentences_pt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WLCwiXl5Vhy",
        "outputId": "be63aadb-d25f-432c-92d4-c7ba210379d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1960408 1960408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences_en[1960406])\n",
        "print(sentences_pt[1960406])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G47elxO9aUU",
        "outputId": "63c58c81-429f-4a7a-e537-5797ecc8a16f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'sitting', 'was', 'closed', 'at', '1050', 'am']\n",
            "['a', 'sessão', 'é', 'suspensa', 'às', '10h50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_sent = []\n",
        "\n",
        "for i in range(len(sentences_en)):\n",
        "  if len(sentences_en[i]) == len(sentences_pt[i]):\n",
        "    s_sent.append([sentences_en[i], sentences_pt[i]])\n",
        "\n",
        "del sentences_en, sentences_pt\n",
        "len(s_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiZVHxb24asa",
        "outputId": "281788ac-c0c4-4ef5-f130-4f9ba518593c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256335"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_sent[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TniRZOrTcYUI",
        "outputId": "62adf04c-860c-42bd-cbb2-460f76cd5755"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['we',\n",
              "  'european',\n",
              "  'socialists',\n",
              "  'are',\n",
              "  'in',\n",
              "  'favour',\n",
              "  'of',\n",
              "  'a',\n",
              "  'market',\n",
              "  'economy',\n",
              "  'with',\n",
              "  'a',\n",
              "  'social',\n",
              "  'purpose'],\n",
              " ['nós',\n",
              "  'socialistas',\n",
              "  'europeus',\n",
              "  'somos',\n",
              "  'a',\n",
              "  'favor',\n",
              "  'de',\n",
              "  'uma',\n",
              "  'economia',\n",
              "  'de',\n",
              "  'mercado',\n",
              "  'de',\n",
              "  'finalidade',\n",
              "  'social']]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences, target_sentences = zip(*choices(s_sent, k=10000))\n",
        "source_sentences = list(source_sentences)\n",
        "target_sentences = list(target_sentences)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NNkoIcdH_xoO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1v9mrSkarX1",
        "outputId": "23a9aa6d-9a94-491e-f2d2-5203aa42ec1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 10000\n"
          ]
        }
      ],
      "source": [
        "print(len(source_sentences), len(target_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(source_sentences[:2])\n",
        "print(target_sentences[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok4ZkRzfBVjd",
        "outputId": "06deee02-518b-44aa-9db3-7c289577a3a0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['this', 'is', 'firstly', 'because', 'the', 'agreement', 'forms', 'part', 'of', 'a', 'broad', 'context', 'of', 'securitarian', 'measures', 'note', 'the', 'programme', 'of', 'illegal', 'phone', 'tapping', 'in', 'the', 'united', 'states', 'on', 'the', 'basis', 'of', 'the', 'socalled', 'fight', 'against', 'terrorism', 'which', 'has', 'been', 'a', 'pretext', 'for', 'unleashing', 'interference', 'and', 'aggression', 'on', 'countries', 'and', 'peoples', 'when', 'us', 'imperialist', 'interests', 'have', 'been', 'at', 'stake'], ['what', 'does', 'it', 'intend', 'to', 'do', 'about', 'this']]\n",
            "[['desde', 'logo', 'porque', 'esse', 'acordo', 'se', 'integra', 'num', 'amplo', 'conjunto', 'de', 'medidas', 'de', 'cariz', 'securitário', 'refirase', 'o', 'programa', 'de', 'escutas', 'telefónicas', 'ilegais', 'nos', 'eua', 'que', 'tem', 'por', 'base', 'a', 'denominada', 'luta', 'contra', 'o', 'terrorismo', 'pretexto', 'para', 'o', 'desencadeamento', 'da', 'ingerência', 'e', 'da', 'agressão', 'a', 'países', 'e', 'povos', 'quando', 'estejam', 'em', 'causa', 'os', 'interesses', 'imperialistas', 'nomeadamente', 'dos', 'eua'], ['que', 'tenciona', 'fazer', 'a', 'comissão', 'a', 'este', 'respeito']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do modelo"
      ],
      "metadata": {
        "id": "DdMW25KbhJkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_exWZNlzZuzY"
      },
      "outputs": [],
      "source": [
        "bitext = []\n",
        "for i in range(len(source_sentences)):\n",
        "  bitext.append(AlignedSent(source_sentences[i], target_sentences[i]))\n",
        "\n",
        "ibm_model = IBMModel1(bitext, 5)  # 5 iterations for training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(round(ibm_model.translation_table['good']['bom'], 3))\n",
        "print(round(ibm_model.translation_table['president']['presidente'], 3))\n",
        "print(round(ibm_model.translation_table['parliament']['parlamento'], 3))\n",
        "print(round(ibm_model.translation_table['european']['europeu'], 3))\n",
        "print(round(ibm_model.translation_table['european'][None], 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ-5T4cJGKma",
        "outputId": "205d85ae-9727-49db-a55e-b3ced01156e9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.798\n",
            "0.763\n",
            "0.92\n",
            "0.882\n",
            "0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação"
      ],
      "metadata": {
        "id": "aMGyiwbehUai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, ibm_model):\n",
        "    translated_sentence = []\n",
        "    for word in sentence:\n",
        "        if word in ibm_model.translation_table:\n",
        "            # Escolher a palavra de destino com maior probabilidade\n",
        "            best_translation = max(ibm_model.translation_table[word], key=ibm_model.translation_table[word].get)\n",
        "            translated_sentence.append(best_translation)\n",
        "        else:\n",
        "            translated_sentence.append(word)  # Palavra desconhecida, mantém original\n",
        "    return \" \".join(translated_sentence)"
      ],
      "metadata": {
        "id": "2TNxC-HlMbCt"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = [['O', 'governo', 'é', 'corrupto']]  # Example reference translation\n",
        "new_sentence = ['the', 'government', 'is',  'corrupt']\n",
        "candidate = translate_sentence(new_sentence, ibm_model)  # Example machine output\n",
        "score = sentence_bleu(reference, candidate)\n",
        "\n",
        "print('Referência: ', ' '.join(reference[0]))\n",
        "print('Tradução do modelo: ', candidate)\n",
        "print('BLEU: ', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6aTK6H0R64H",
        "outputId": "36227c38-79d4-4a29-b02e-44bda50eeda9"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Referência:  O governo é corrupto\n",
            "Tradução do modelo:  o governo é corrupto\n",
            "BLEU:  8.614911585158347e-232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "reference = []\n",
        "candidate = ''\n",
        "\n",
        "for i in range(len(source_sentences)):\n",
        "  reference = [target_sentences[i]]\n",
        "  candidate = translate_sentence(source_sentences[i], ibm_model)\n",
        "  score += sentence_bleu(reference, candidate)\n",
        "\n",
        "mean_BLEU = score / len(source_sentences)\n",
        "\n",
        "print('mean_score: ', mean_BLEU)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmT37XDYV5_1",
        "outputId": "55612aa3-69b5-4cc6-d8e5-ebd2b27630cf"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_score:  1.3813326996009179e-157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZonYp7_KX0z"
      },
      "source": [
        "# Codificador-Decodificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "hw2YV9RSvg4i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Seq2SeqEncoder(d2l.Encoder):\n",
        "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n",
        "\n",
        "    def call(self, X, *args):\n",
        "        # X shape: (batch_size, num_steps)\n",
        "        embs = self.embedding(tf.transpose(X))\n",
        "        # embs shape: (num_steps, batch_size, embed_size)\n",
        "        outputs, state = self.rnn(embs)\n",
        "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
        "        # state shape: (num_layers, batch_size, num_hiddens)\n",
        "        return outputs, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "_erq476KKX0z"
      },
      "outputs": [],
      "source": [
        "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
        "batch_size, num_steps = 4, 9\n",
        "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
        "X = tf.zeros((batch_size, num_steps))\n",
        "enc_outputs, enc_state = encoder(X)\n",
        "d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "giASHjSxKX00"
      },
      "outputs": [],
      "source": [
        "d2l.check_len(enc_state, num_layers)\n",
        "d2l.check_shape(enc_state[0], (batch_size, num_hiddens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "LSm5hCc2KX00"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqDecoder(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def init_state(self, enc_all_outputs, *args):\n",
        "        return enc_all_outputs\n",
        "\n",
        "    def call(self, X, state):\n",
        "        # X shape: (batch_size, num_steps)\n",
        "        # embs shape: (num_steps, batch_size, embed_size)\n",
        "        embs = self.embedding(tf.transpose(X))\n",
        "        enc_output, hidden_state = state\n",
        "        # context shape: (batch_size, num_hiddens)\n",
        "        context = enc_output[-1]\n",
        "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
        "        context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))\n",
        "        # Concat at the feature dimension\n",
        "        embs_and_context = tf.concat((embs, context), -1)\n",
        "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
        "        outputs = tf.transpose(self.dense(outputs), (1, 0, 2))\n",
        "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
        "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
        "        return outputs, [enc_output, hidden_state]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "vYe0HwpOKX00"
      },
      "outputs": [],
      "source": [
        "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
        "state = decoder.init_state(encoder(X))\n",
        "dec_outputs, state = decoder(X, state)\n",
        "d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\n",
        "d2l.check_len(state[1], num_layers)\n",
        "d2l.check_shape(state[1][0], (batch_size, num_hiddens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Ix5iRFbxKX00"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(d2l.EncoderDecoder):\n",
        "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        Y_hat = self(*batch[:-1])\n",
        "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Adam optimizer is used here\n",
        "        return tf.keras.optimizers.Adam(learning_rate=self.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "2oVjmiFqKX00"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(Seq2Seq)\n",
        "def loss(self, Y_hat, Y):\n",
        "    l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n",
        "    mask = tf.cast(tf.reshape(Y, -1) != self.tgt_pad, tf.float32)\n",
        "    return tf.reduce_sum(l * mask) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "iLpkBN80KX00",
        "outputId": "708c5837-130a-4f23-e4b7-e88e5044c3b7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'train_dataloader'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-eefab462d4ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_clip_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'train_dataloader'"
          ]
        }
      ],
      "source": [
        "tgt_vocab = d2l.Vocab(target_sentences, min_freq=2,\n",
        "                      reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
        "with d2l.try_gpu():\n",
        "    encoder = Seq2SeqEncoder(\n",
        "        len(source_sentences), embed_size, num_hiddens, num_layers, dropout)\n",
        "    decoder = Seq2SeqDecoder(\n",
        "        len(target_sentences), embed_size, num_hiddens, num_layers, dropout)\n",
        "    model = Seq2Seq(encoder, decoder, tgt_pad=tgt_vocab['<pad>'], lr=0.005)\n",
        "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "6zREhTUbKX00"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.EncoderDecoder)\n",
        "def predict_step(self, batch, device, num_steps,\n",
        "                 save_attention_weights=False):\n",
        "    src, tgt, src_valid_len, _ = batch\n",
        "    enc_all_outputs = self.encoder(src, src_valid_len, training=False)\n",
        "    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
        "    outputs, attention_weights = [tf.expand_dims(tgt[:, 0], 1), ], []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = self.decoder(outputs[-1], dec_state, training=False)\n",
        "        outputs.append(tf.argmax(Y, 2))\n",
        "        # Save attention weights (to be covered later)\n",
        "        if save_attention_weights:\n",
        "            attention_weights.append(self.decoder.attention_weights)\n",
        "    return tf.concat(outputs[1:], 1), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu(pred_seq, label_seq, k):\n",
        "    \"\"\"Compute the BLEU.\"\"\"\n",
        "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
        "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "    for n in range(1, min(k, len_pred) + 1):\n",
        "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "        for i in range(len_label - n + 1):\n",
        "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
        "        for i in range(len_pred - n + 1):\n",
        "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
        "                num_matches += 1\n",
        "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
        "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "    return score"
      ],
      "metadata": {
        "id": "aTV1sqMIu1SY"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwhL54tgKX00",
        "outputId": "d0ebfcfb-3372-4b77-d867-4d8292ef6148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go . => ['<unk>', '!'], bleu,0\n",
            "i lost . => ['je', 'suis', '<unk>', '.'], bleu,0.25\n",
            "he's calm . => ['je', 'suis', '<unk>', '.'], bleu,0.25\n",
            "i'm home . => ['je', 'suis', '<unk>', '.'], bleu,0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
        "ports = ['vai .', 'eu perdi .', 'ele está calmo .', 'estou em casa .']\n",
        "preds, _ = model.predict_step(\n",
        "    data.build(engs, ports), d2l.try_gpu(), data.num_steps)\n",
        "for en, pt, p in zip(engs, ports, preds):\n",
        "    translation = []\n",
        "    for token in data.tgt_vocab.to_tokens(p):\n",
        "        if token == '<eos>':\n",
        "            break\n",
        "        translation.append(token)\n",
        "    print(f'{en} => {translation}, bleu,'\n",
        "          f'{sentence_bleu([pt.split()], translation, weights=(1, 0, 0, 0))}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "NoIA-3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}